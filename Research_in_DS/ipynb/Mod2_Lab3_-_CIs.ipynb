{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Module 2, Lab 3: Confidence Intervals\n=====================================\n\nThis lab provides a brief extension on the previous lab, pointing out\nsomething you may have missed in the results before. Be sure to have the\ndata from the previous lab open in your computer.\n\nMoving Beyond *p* &lt; .05\n==========================\n\nIn the previous lab, we tested the null hypothesis. We specifically\nasked: \"what percentage of the time could you get your result if the\nnull were true?\" We called that percentage a *p*-value. The lower the\n*p*-value, the less easily it is that you could get that result if the\nnull were true. In other words, a low *p*-value means that the data and\nthe null are incompatible. Since we actually did get the data, we reject\nthe null. This is a useful way to test the null hypothesis, but it's not\nthe only way.\n\nConsider one weakness of the procedure outlined above. The null\nhypothesis states that the sample result is *entirely* due to chance; in\nthe population the effect is completely absent (e.g., in the attitude\nexample in the previous lab, the null said that the average attitude was\nexactly zero, or *H*<sub>0</sub> : *μ* = 0, meaning that any non-zero\nresult observed in the sample was entirely due to chance). When we\nreject the null hypothesis, we are rejecting the premise that the\nresults are entirely due to chance. That's about it.\n\nIn other words, rejecting the null hypothesis tells you what the effect\nis *not* (i.e., not zero, not due to chance). You aren't saying what the\neffect *is.* If you want to estimate what is going on in the population,\nwe need to do more, to use our sample estimate as a starting point and\nadd some measure of uncertainty.\n\nConfidence Intervals\n====================\n\nThis is the goal of confidence intervals. A confidence interval simply\ntakes your sample result and puts it at the center of a ranged estimate.\nWe know that the sample result has error in it; it's an imperfect\nestimate of the population. However, we can estimate *how much error*\nand can use that to create a range around our sample estimate. I will\nshow you an example first, and then I will show you how it works.\n\nLoad the `attitude` data from the previous lab (re-run the code from the\nprevious lab if you need). We observed a sample average attitude of 2.23\n(on a scale from -5 to +5):"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nattitude = np.array(pd.read_csv('attitude.csv'))[:,1]\nnp.mean(attitude)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "2.234095719379859"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This was clearly in the positive range, and it was significant, meaning\nthat we could reject the null hypothesis that it was *zero* in the\npopulation (i.e., sample result a fluke due to chance).\n\nHowever, we can also say on the basis of our sample data with 95%\nconfidence that the population average is somewhere in the range from\n1.73 to 2.74. This is often written: \"95% CI \\[1.73, 2.74\\].\" Look back\nat the last lab: "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from scipy import stats\ndef t_one_sample(samp, mu = 0.0, alpha = 0.05):\n    '''Function for two-sided one-sample t test'''\n    t_stat = stats.ttest_1samp(samp, mu)\n    scale = np.std(samp)\n    loc = np.mean(samp)\n    ci = stats.t.cdf(alpha/2, len(samp), loc=mu, scale=scale)\n    print('Results of one-sample two-sided t test')\n    print('Mean         = %4.3f' % loc)\n    print('t-Statistic  = %4.3f' % t_stat[0])\n    print('p-value      < %4.3e' % t_stat[1])\n    print('On degrees of freedom = %4d' % (len(samp) - 1))\n    print('Confidence Intervals for alpha =' + str(alpha))\n    print('Lower =  %4.3f Upper = %4.3f' % (loc - ci, loc + ci))\n    \nt_one_sample(attitude)    ",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Results of one-sample two-sided t test\nMean         = 2.234\nt-Statistic  = 10.725\np-value      < 2.881e-18\nOn degrees of freedom =   99\nConfidence Intervals for alpha =0.05\nLower =  1.729 Upper = 2.739\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice the confidence interval:\n\n`Lower       1.729`  \n`Upper       2.739`\n\nThis is **much** more useful than our *p* &lt; .05 finding. Think about\nit. *p* &lt; .05 tells us that we can reject the null hypothesis (which\nstated *μ* = 0). The 95% confidence interval tells you that it's\n*probably not zero* **and** that it's probably between 1.73 and 2.74. It\nboth rejects the null and tells you where the population value likely\nis. It's rejecting the null and giving you additional information.\n\nThe CI also tells you how precise our result is. The two bounds on the\nCI are close together, indicating I have a very good read on where the\npopulation mean is. On a -5 to +5 scale, the CI tells me we are very\nlikely in the \"mid-range positive\" territory on our attitude scale.\n\nConsider what would happen if the range were bigger. Imagine, for\nexample, that you saw this instead:\n\n`Low95CI        0.496`  \n`High95CI       4.092`\n\nOn our -5 to +5 attitude scale, this would be telling you that you are\n95% confident that the mean of the population is somewhere between .50\n(almost zero) and 4.09 (incredibly strongly positive). This tells us\nsomething (we are still sure it's in the positive range; i.e.,\nsignificant or not zero), but it tells you little else. It's almost\nuseless for decision making. Yes, people feel positive, on average.\nHowever, in this case, your result is so imprecise that you really can't\nsay much beyond \"it's not zero.\"\n\nTherefore, I always advocate for confidence intervals. They are given to\nyou in almost all statistical output in any statistical or data science\ntool. They let you quantify exactly how much certainty you have in your\nresult.\n\nWhere does this range come from? We start with the sample estimate and\nadd or subtract a margin of error. The margin of error, in turn,\nconsists of the standard error (discussed in the sampling lab)\nmultiplied by a scaling constant. For our mean, this looks as follows:\n\n$$CI = \\bar{x} \\pm \\left (SE\\right )\\left (constant\\right )$$\n The purpose of the constant is to make the range big enough that you\nare 95% confident it contains the true population value. Recall that the\nstandard error (SE) tells you how much error you expect in a typical\nsample, or how much error you expect on average. However, that would not\ngive you a range that would engender 95% confidence. If you want the\nrange to be large enough **that it contains the population value 95% of\nthe time**, you need to add the scaling constant. (In case you've had\nsome statistics training: this constant, it turns out, is actually the\n\"critical value\" for the *t*-test --not discussed in this class, but you\nmay have learned it in a previous statistics course)."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}