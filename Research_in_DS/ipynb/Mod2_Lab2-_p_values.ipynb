{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Module 2, Lab 2: *p*-values\n===========================\n\nIn this lab, we will explore how the basics of null hypothesis\nsignificance testing work. Although you may have examined this in a\nprevious course, we will review the concepts of *p*-values and tests of\nstatistical significance with an emphasis on their application in\nresearch.\n\nThe Null Hypothesis\n===================\n\nFirst, we briefly review what the null hypothesis is. Recall from the\nprevious lab that the results that come from samples are only mere\nestimates of the population. Because they are estimates, the statistics\nthey produce will differ somewhat from their population counterparts.\nFor example, the correlation between engagement in a sample may be *r* =\n.2 even when the correlation between those same variables in the\npopulation is something smaller, such as .10 or even 0. This can cause\napparent relationships and effects to appear in *samples* when none in\nfact exists in the population. This idea--that the effect/association is\n*zero* in the population--is called the null hypothesis. By implication,\nany effect/association seen in the *sample* must be entirely due to the\nrandom chance of \"sampling error.\" In other words, the null hypothesis\nclaims that the sample result is a random fluke.\n\nLet's explore an application of this. Imagine we want to compare males\nand females in terms of their interest in a given product. Imagine, for\na moment, that *the two groups have identical interest* (in the\npopulation)...that is, there is no difference between the groups.\nNevertheless, if we take a sample of males and a sample of females, the\nerror in our estimations will cause a difference to appear.\n\nImagine that *both* males and females had an interest level averaging at\n5, with a standard deviation of 3."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# set seed to make random number generation reproducible\nimport numpy as np\nimport numpy.random as nr\nnr.seed(515212)\n\n#collect a sample of 100 males\nmales = nr.normal(5, 3, 100)\n\n#collect a sample of 100 females\nfemales = nr.normal(5, 3, 100)\n\nprint(np.mean(males))\nprint(np.mean(females))",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "5.337790559409466\n4.8650214885878595\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see here that our two groups have different sample results. Let's see\nhow large the difference is:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.mean(males)-np.mean(females)",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "0.4727690708216068"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see here that the females are almost 1/2 of a point higher than the\nmales. If you saw this data in an organization where you were working,\nyou might be tempted to think you'd discovered a female preference for\nyour product. However, in fact, we *know* in this case that this is\nnonsense as we *know* (because we wrote the Python code simulating this data)\nthat *both* groups were random samples from a population with a mean of\n5 and a standard deviation of 3. If their means are both 5.0 (exactly)\nin the population, why did the females score higher in the samples? It's\nsimple: sampling error. That is, the difference is **entirely** due to\nrandom error in the samples, not any real difference in the population.\nWe have discovered a fluke in some sample data, nothing more.\n\nThis is a case of the \"null hypothesis.\" In this case, the means are\n*equal* in the population. We write the null hypothesis as\n*H*<sub>0</sub> and it is always a statement that the size of the effect\nin the population is zero. In this case, we are testing the difference\nbetween the averages ($\\mu ' s$), stating that the *difference between\nthem is zero*:\n\n$$H_0 :\\ \\mu_{male} - \\mu_{female} = 0$$\n\nHowever, to reiterate what we saw above, *when we looked at our\nsamples,* we saw there was a difference:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.mean(males)-np.mean(females)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "0.4727690708216068"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So, in conclusion, the null hypothesis says that whatever effect you are\nstudying is *zero* in the population and *your sample results are due to\nrandom chance.*\n\nThis possibility looms ominously over every research finding based on\nsamples data. How do we know that the effects we trust every day (the\neffect of medicine, tested leadership practices, etc.) are real and not\njust flukes due to random sampling error? We need to find a way to test\nthe null hypothesis and see if we can reject this possibility.\n\nNull Hypothesis Significance Test: The *p*-Value\n================================================\n\nTo test the null hypothesis, we simply ask: *if the null hypothesis were\ntrue, what percentage of the time would I get this result this large?*\nThe answer to that question is called a *p*-value.\n\nThere is a lot of confusion about *p*-values, so let's review:\n\n-   *p*-values represent how often you could get a result as big as you\n    did *if the null were true*\n-   *p*-values therefore represent how easy/hard it would be to get a\n    result by chance\n-   *p*-values do **not** tell you the probability that the result is\n    due to chance; only the probability of seeing *your result* if the\n    null were true\n-   If the *p*-value for a result is small, it would be rare to get that\n    result by chance (i.e., if the null were true)\n-   If the *p*-value for a result is large, it would be common to get\n    that result by chance (i.e., if the null were true)\n-   Conclusion: the *p*-value is a measure of \"incompatibility\" between\n    your result and the null. If the *p*-value is small, one of the two\n    (the data, or the null) is likely wrong. We opt to trust our data\n    and reject the null.\n\nTo be clear: the *p*-value is a backwards way of testing the null\nhypothesis. We would love to know the *probability* that the null\nhypothesis is true--the probability that the results *are* due to\nchance--but we cannot know that. You will often hear the *p*-value\ndescribed this way, but that is **very wrong**.\n\nSo, to repeat, the *p-value states the probability of getting **your\nresult** if the null is true*. It is essentially a statement of\nincompatibility between your data and the null. A small *p*-value\n(typically, less than 5% or \"&lt; . 05\") tells you that the data and\nnull are highly incompatible. Since you did in fact observe the data,\nyou conclude the null hypothesis is false. This is the only use for the\n*p*-value.\n\nWhere do *p*-Values Come From?\n==============================\n\nWhere does a *p*-value come from? Every data situation is different, but\nthe process in so-called \"frequentist\" statistics is always the same\"\"\n\n1.  Observe data and examine result\n2.  Compute the appropriate \"test statistic\" for that result (e.g., *t*\n    test, *z* test, *χ*<sup>2</sup> test, *F* test, *q* test etc.).\n3.  Observe how often you could get the observed test statistic if the\n    null hypothesis was true. This is the *p*-value\n4.  If the *p*-value is less than .05, declare the result \"significant\"\n    and reject the null hypothesis\n\nLet's see this in action. For this example, I will use a \"one-sample\n*t*-test\", as the math is easier.\n\nImagine we assess people's impressions of a training given in an\norganization. We assess attitudes toward the training on a -5 (very\nnegative) to +5 (very positive) scale (zero = neutral opinion).\n\nThe question is whether people have a positive or negative attitude\ntoward the training, on average. Let's imagine that they actually have a\npositive attitude, that in the population the mean is really 2.4 (i.e.,\n*μ* = 2.4) with a standard deviation of 2.0. This is a simulated example\n(in real life, you would have no idea what the population value is:\nthat's why you're doing research). Still, by showing you a simulated\nexample, we can see how the procedure works.\n\nWhat would the null hypothesis be, here? Well, the null hypothesis\nalways states that the effect is absent. In this case, an \"effect\" would\nbe a non-zero attitude. Thus, in this case, *H*<sub>0</sub> : *μ* = 0.\n\nLet's pull a random sample of 100 scores from that population."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(4455)\nattitude = nr.normal(2.4, 2.0, 100)",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "What are the mean and SD in our sample?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(np.mean(attitude))\n\nprint(np.std(attitude))",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "2.234095719379859\n2.0725742818363613\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### So, our null hypothesis is that the mean is zero\n(*H*<sub>0</sub> : *μ* = 0) but our sample result disagrees with that\n(sample mean = 2.23).\n\nDoes this *sample* gives us enough evidence to reject the null?\n\nTo answer that question, we calculate a test statistic. In this case\n(one group, sample mean), we conduct a one-group *t*-test for means. (As\nyou progress in your data science and statistics knowledge, you will\nlearn when to use different kinds of tests.)\n\nIn the *t*-test, we compare the size of the difference between our\nobserved result and the null hypothesis, divided by what you would\ntypically expect by chance (i.e., standard error):\n\n$$t=\\frac{result - null }{chance}$$\n\nSince our sample result is a sample mean ($\\\\bar{x}$), and we know the\n$$t = \\frac{\\bar{x}-H_0}{\\frac{SD}{\\sqrt{n}}}$$\n\nWe can plug in our numbers easily:\n\n$$t = \\frac{\\bar{x}-H_0}{\\frac{SD}{\\sqrt{n}}} =  \\frac{2.234-0}{\\frac{2.073}{\\sqrt{100}}} = 10.8$$\n The test assesses how much the data disagree with the null (i.e., the\neffect; top of fraction) compared to what you would typically expect by\nchance (bottom of fraction). Thus, we can literally read the result as\nsaying \"our effect was 10.8 times greater than you would typically\nexpect by chance.\" That sounds pretty good for our effect and pretty bad\nfor the null hypothesis.\n\nIt is convenient that the *t*-test works this way. However, truth be\ntold, the test statistic need not have *any* intuitive meaning. To get\nour *p*-value, the only thing we need to do is assess how rare our\nresult would be if the null hypothesis was true. Thus, it doesn't really\nmatter if we can interpret the *p*-value directly. We simply need to\nknow where *t*-test results tend to be when the null is true, and then\nwe can see how rare a score of 10.8 would be in that situation, giving\nus our *p*-value.\n\nThis is an easy question to answer. Statisticians have mapped out the\nexact behavior of each test statistic when the null hypothesis is true\n(or as we often say, \"under the null\"). We know, for example, that if\nthe null hypothesis is true, that the *t*-test will be close to zero\n(almost always within $$3 points of zero). So, what is our *p*-value? If\nthe null were true, how often could we get *t*-test result as big as\n10.8?\n\nUsing Python\n=======\n\nWith a bit of programming, Python will do all of this work for you:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from scipy import stats\ndef t_one_sample(samp, mu = 0.0, alpha = 0.05):\n    '''Function for two-sided one-sample t test'''\n    t_stat = stats.ttest_1samp(samp, mu)\n    scale = np.std(samp)\n    loc = np.mean(samp)\n    ci = stats.t.cdf(alpha/2, len(samp), loc=mu, scale=scale)\n    print('Results of one-sample two-sided t test')\n    print('Mean         = %4.3f' % loc)\n    print('t-Statistic  = %4.3f' % t_stat[0])\n    print('p-value      < %4.3e' % t_stat[1])\n    print('On degrees of freedom = %4d' % (len(samp) - 1))\n    print('Confidence Intervals for alpha =' + str(alpha))\n    print('Lower =  %4.3f Upper = %4.3f' % (loc - ci, loc + ci))\n    \nt_one_sample(attitude)    ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Results of one-sample two-sided t test\nMean         = 2.234\nt-Statistic  = 10.725\np-value      < 2.881e-18\nOn degrees of freedom =   99\nConfidence Intervals for alpha =0.05\nLower =  1.729 Upper = 2.739\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The key information is from this function is:\n`t statistic = 10.7, df = 99, p-value < 2.9e-18`. Notice that the *p*-value is displayed in scientific notation. `2.9e-18` is scientific notation:\n2.9 x 10<sup>-18</sup> and means the same as 0.0000000000000000029. This\nis clearly less than .05 so we can reject the null hypothesis and\nconclude that the positive attitude observed among our participants was\nnot a statistical fluke but likely a real trend in the population.\n\n\n### For Illustration Purposes\n\nHow did Statsmodels compute that *p*-value? I will illustrate.\n\nI start with a plot of all the *t*-test results (for sample size of 100)\nyou would expect **if the null hypothesis was true.** We know this,\nthanks to mathematicians.\n![](img/unnamed-chunk-8-1.png)\n\nThe bell curve above illustrates all the possible *t*-test results one\nwould expect when the null is true and their respective probabilities.\nWe see here that most results are within about +/- 3 points from zero.\nWhere is our result? Let's add it to the plot.\n\n![](img/unnamed-chunk-9-1.png)\n\nAs we see, our result is out among values that are very, very rare under\nthe null hypothesis. It appears that our data disagree the null\nhypothesis. When the null is true, we should be getting *t*-test results\ndown in the center of the bell curve (approximately ± 3), but we didn't.\nWe were up at 10.8.\n\nTo find the *p*-value, we simply ask what percentage of our *t*-curve is\nout that far. In other words, what proportion of the bell curve extends\nout beyond the red line? What is the area \"in the upper tail\"?\n\nWe can compute the p-value as $1 - cdf$, for the t-statistic, where $cdf$ is the cumulative density function. The statsmodels `t.cdf()` function computes the cdf given the t-statistic and the degrees of freedom; $n − 1 = 100 − 1 = 99$:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from scipy.stats import t\n1 - t.cdf(10.8, df = 99, loc=0, scale=1)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "0.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This result is saying there is \"zero\" probability of getting a result this big if\nthe null were true; i.e., *p* = 0. In reality, *p* values are never zero\nbut can get infinitely small. In this case the a tiny number is rounded to 0.\n\nThis is called a on-tailed *p*-value. We actually, however, need to\ndouble it. The reason we need to double it is that our null hypothesis\nwas that *μ* = 0. That is, the null is false if our result is\nsignificantly *larger* than zero (a positive attitude) or significantly\n*smaller* than zero (a negative attitude). This is consistent with how\nwe asked our question: \"do people have positive or negative attitudes?\"\nIn other words, we did not test a directional prediction; we would be\ninterested in \"finding\" something regardless of the direction of the\neffect. Since the *p*-value is the probability of getting an effect\n\"this large\" and we do not care about the direction, it actually exists\non both sides of the distribution (a negative attitude would have given\nus a negative *t*-score):\n\n![](img/unnamed-chunk-11-1.png)\n\nThus, we have to double our *p*-value. This is standard practice any\ntime you would be willing to declare the result significant **regardless\nof the direction**. We call this a *two-tailed p-value*.\n\nIf this explanation is confusing, you can also understand it a slightly\ndifferent way: by testing *H*<sub>0</sub> : *μ* = 0, you are really\nasking whether *μ* &lt; 0 or whether *μ* &gt; 0. You are essentially\nasking two separate questions of the data. You need to double your\n*p*-value.\n\nThis is almost always what you want. We almost always want to be able to\ndeclare a result significant if the effect is large, regardless of\nwhether the direction of the result matches our intuition or not. For\nexample, if an intervention to increase productivity backfires and\ndecreases productivity, we want to know that just as much as we want to\nknow if it works.\n\nThus, we almost always double the *p*-value for this reason. It is true\nthat it makes it a little harder to get a significant result (less than\n.05), but we can extract more meaning from the result. It's worth it.\n\nNote: our doubled *p*-value here is still essentially zero:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "2.0 * (1 - t.cdf(10.8, df = 99, loc=0, scale=1))",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "0.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}